{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nDataset Preparation Script for Phi-4 Math Fine-tuning\nDownloads, processes, and merges NuminaMath-TIR, NuminaMath-CoT, and AIME datasets.\nLogs statistics to Weights & Biases.\n\"\"\"\n\nimport json\nimport re\nimport random\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom typing import Optional, Dict, List, Any\n\nimport wandb\nfrom datasets import load_dataset, Dataset\nfrom tqdm import tqdm\nfrom kaggle_secrets import UserSecretsClient\n\n# Get W&B API key from Kaggle secrets\nsecrets = UserSecretsClient()\nwandb_api_key = secrets.get_secret(\"WANDB_API_KEY\")\n\n# Login to W&B\nimport wandb\nwandb.login(key=wandb_api_key)\n\n# === Configuration ===\nCONFIG = {\n    \"output_dir\": \"data\",\n    \"sft_output\": \"sft_dataset.jsonl\",\n    \"grpo_output\": \"grpo_dataset.jsonl\",\n    \n    # Dataset sizes\n    \"numina_cot_sample\": 40000,\n    \"aime_min_year\": 2000,\n    \n    # Difficulty distribution (must sum to 1.0)\n    \"difficulty_split\": {\n        \"easy\": 0.20,\n        \"medium\": 0.30,\n        \"hard\": 0.50\n    },\n    \n    # W&B\n    \"wandb_project\": \"phi4-math-data\",\n    \"wandb_run_name\": \"dataset-preparation\",\n    \n    # Random seed\n    \"seed\": 42\n}\n\n# Prompt template for Phi-4\nPROMPT_TEMPLATE = \"\"\"<|user|>\n{problem}\n<|end|>\n<|assistant|>\n{solution}\n<|end|>\"\"\"\n\nGRPO_TEMPLATE = \"\"\"<|user|>\n{problem}\n<|end|>\n<|assistant|>\n\"\"\"\n\n\ndef set_seed(seed: int):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    random.seed(seed)\n\n\ndef extract_answer(solution: str) -> Optional[str]:\n    \"\"\"Extract boxed answer from solution.\"\"\"\n    patterns = [\n        r'\\\\boxed\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}',\n        r'\\\\fbox\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}',\n        r'[Tt]he\\s+(?:final\\s+)?answer\\s+is[:\\s]+\\$?([^\\$\\n,\\.]+)',\n        r'[Aa]nswer[:\\s]+\\$?([^\\$\\n,\\.]+)',\n    ]\n    \n    for pattern in patterns:\n        matches = re.findall(pattern, solution)\n        if matches:\n            return matches[-1].strip()\n    return None\n\n\ndef has_code_blocks(text: str) -> bool:\n    \"\"\"Check if text contains Python code blocks.\"\"\"\n    return bool(re.search(r'```python', text, re.IGNORECASE))\n\n\ndef classify_difficulty(example: Dict, source: str) -> str:\n    \"\"\"\n    Classify problem difficulty based on source and solution length.\n    \n    Rules:\n    - AIME, olympiads, IMO ‚Üí hard\n    - AMC, competition_math level 4-5 ‚Üí medium/hard\n    - Basic math, short solutions ‚Üí easy\n    \"\"\"\n    solution = example.get(\"solution\", \"\")\n    solution_len = len(solution)\n    \n    # Source-based classification\n    source_lower = source.lower()\n    problem_source = example.get(\"source\", \"\").lower()\n    \n    # Hard sources\n    hard_keywords = [\"aime\", \"imo\", \"olympiad\", \"usamo\", \"putnam\", \"hmmt\"]\n    if any(kw in source_lower or kw in problem_source for kw in hard_keywords):\n        return \"hard\"\n    \n    # Medium sources\n    medium_keywords = [\"amc\", \"mathcounts\", \"competition\"]\n    if any(kw in source_lower or kw in problem_source for kw in medium_keywords):\n        return \"medium\" if solution_len < 2000 else \"hard\"\n    \n    # Length-based heuristic for others\n    if solution_len < 500:\n        return \"easy\"\n    elif solution_len < 1500:\n        return \"medium\"\n    else:\n        return \"hard\"\n\n\ndef format_example(problem: str, solution: str) -> str:\n    \"\"\"Apply prompt template to create formatted text.\"\"\"\n    return PROMPT_TEMPLATE.format(\n        problem=problem.strip(),\n        solution=solution.strip()\n    )\n\n\ndef normalize_example(example: Dict, source: str) -> Dict[str, Any]:\n    \"\"\"Normalize example to unified schema.\"\"\"\n    # Handle different field names\n    problem = example.get(\"problem\") or example.get(\"question\") or \"\"\n    solution = example.get(\"solution\") or example.get(\"answer\") or \"\"\n    \n    # For TIR, solution might be in different field\n    if not solution and \"messages\" in example:\n        # Extract from chat format if present\n        messages = example.get(\"messages\", [])\n        for msg in messages:\n            if msg.get(\"role\") == \"assistant\":\n                solution = msg.get(\"content\", \"\")\n                break\n    \n    answer = extract_answer(solution)\n    \n    return {\n        \"text\": format_example(problem, solution),\n        \"problem\": problem.strip(),\n        \"solution\": solution.strip(),\n        \"answer\": answer,\n        \"source\": source,\n        \"difficulty\": classify_difficulty(example, source),\n        \"has_code\": has_code_blocks(solution),\n    }\n\n\ndef download_numina_tir() -> List[Dict]:\n    \"\"\"Download and process NuminaMath-TIR dataset.\"\"\"\n    print(\"\\nüì• Downloading NuminaMath-TIR...\")\n    \n    try:\n        ds = load_dataset(\"AI-MO/NuminaMath-TIR\", split=\"train\")\n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Error loading NuminaMath-TIR: {e}\")\n        print(\"   Trying alternative loading...\")\n        ds = load_dataset(\"AI-MO/NuminaMath-TIR\", split=\"train\", trust_remote_code=True)\n    \n    examples = []\n    for item in tqdm(ds, desc=\"   Processing TIR\"):\n        normalized = normalize_example(item, \"numina_tir\")\n        if normalized[\"problem\"] and normalized[\"solution\"]:\n            examples.append(normalized)\n    \n    print(f\"   ‚úÖ Loaded {len(examples)} examples from NuminaMath-TIR\")\n    return examples\n\n\ndef download_numina_cot(sample_size: int) -> List[Dict]:\n    \"\"\"Download and process NuminaMath-CoT dataset with sampling.\"\"\"\n    print(f\"\\nüì• Downloading NuminaMath-CoT (sampling {sample_size})...\")\n    \n    try:\n        ds = load_dataset(\"AI-MO/NuminaMath-CoT\", split=\"train\")\n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Error loading NuminaMath-CoT: {e}\")\n        ds = load_dataset(\"AI-MO/NuminaMath-CoT\", split=\"train\", trust_remote_code=True)\n    \n    # Process all first\n    all_examples = []\n    for item in tqdm(ds, desc=\"   Processing CoT\"):\n        normalized = normalize_example(item, \"numina_cot\")\n        if normalized[\"problem\"] and normalized[\"solution\"]:\n            all_examples.append(normalized)\n    \n    # Sample if needed\n    if len(all_examples) > sample_size:\n        examples = random.sample(all_examples, sample_size)\n    else:\n        examples = all_examples\n    \n    print(f\"   ‚úÖ Loaded {len(examples)} examples from NuminaMath-CoT (from {len(all_examples)} total)\")\n    return examples\n\n\ndef download_aime(min_year: int) -> tuple:\n    \"\"\"Download and process AIME dataset, filtering by year.\"\"\"\n    print(f\"\\nüì• Downloading AIME (year >= {min_year})...\")\n    \n    try:\n        ds = load_dataset(\"di-zhang-fdu/AIME_1983_2024\", split=\"train\")\n    except Exception as e:\n        print(f\"   ‚ùå Could not load AIME dataset: {e}\")\n        return [], []\n    \n    print(f\"   Columns: {ds.column_names}\")\n    \n    examples = []\n    grpo_examples = []\n    \n    for item in tqdm(ds, desc=\"   Processing AIME\"):\n        year = item.get(\"Year\")\n        \n        # Filter by year\n        if year is None or year < min_year:\n            continue\n        \n        problem = item.get(\"Question\", \"\")\n        answer = str(item.get(\"Answer\", \"\"))\n        \n        if not problem:\n            continue\n        \n        # AIME dataset only has answers, no solutions\n        # For SFT, we'd need solutions - skip SFT examples\n        # For GRPO, we only need problem + answer ‚úì\n        \n        grpo_examples.append({\n            \"prompt\": GRPO_TEMPLATE.format(problem=problem.strip()),\n            \"problem\": problem.strip(),\n            \"answer\": answer.strip(),\n            \"year\": year,\n            \"source\": \"aime\",\n            \"problem_id\": item.get(\"ID\", \"\"),\n        })\n    \n    # AIME has no solutions, so no SFT examples from this dataset\n    print(f\"   ‚úÖ Loaded {len(grpo_examples)} AIME GRPO examples (year >= {min_year})\")\n    print(f\"   ‚ö†Ô∏è AIME has no solutions, only used for GRPO\")\n    \n    return [], grpo_examples  # Empty SFT, only GRPO\n\n\ndef balance_by_difficulty(\n    examples: List[Dict],\n    target_split: Dict[str, float]\n) -> List[Dict]:\n    \"\"\"\n    Balance dataset to achieve target difficulty distribution.\n    Uses stratified sampling.\n    \"\"\"\n    print(\"\\n‚öñÔ∏è Balancing difficulty distribution...\")\n    \n    # Group by difficulty\n    by_difficulty = defaultdict(list)\n    for ex in examples:\n        by_difficulty[ex[\"difficulty\"]].append(ex)\n    \n    # Current counts\n    total = len(examples)\n    current_dist = {k: len(v) / total for k, v in by_difficulty.items()}\n    print(f\"   Current distribution: { {k: f'{v:.1%}' for k, v in current_dist.items()} }\")\n    print(f\"   Target distribution:  { {k: f'{v:.1%}' for k, v in target_split.items()} }\")\n    \n    # Calculate target counts\n    # Use the limiting factor (difficulty with least relative examples)\n    ratios = {}\n    for diff, target_pct in target_split.items():\n        available = len(by_difficulty[diff])\n        needed_for_full = total * target_pct\n        ratios[diff] = available / needed_for_full if needed_for_full > 0 else float('inf')\n    \n    min_ratio = min(ratios.values())\n    final_total = int(total * min_ratio)\n    \n    # Sample each difficulty\n    balanced = []\n    for diff, target_pct in target_split.items():\n        target_count = int(final_total * target_pct)\n        available = by_difficulty[diff]\n        \n        if len(available) >= target_count:\n            sampled = random.sample(available, target_count)\n        else:\n            sampled = available  # Take all if not enough\n            print(f\"   ‚ö†Ô∏è Not enough {diff} examples: {len(available)} < {target_count}\")\n        \n        balanced.extend(sampled)\n    \n    # Shuffle\n    random.shuffle(balanced)\n    \n    # Verify final distribution\n    final_dist = defaultdict(int)\n    for ex in balanced:\n        final_dist[ex[\"difficulty\"]] += 1\n    \n    print(f\"   Final distribution: { {k: f'{v/len(balanced):.1%} ({v})' for k, v in final_dist.items()} }\")\n    print(f\"   Final dataset size: {len(balanced)}\")\n    \n    return balanced\n\n\ndef compute_statistics(examples: List[Dict]) -> Dict[str, Any]:\n    \"\"\"Compute dataset statistics for logging.\"\"\"\n    stats = {\n        \"total_examples\": len(examples),\n        \"by_source\": defaultdict(int),\n        \"by_difficulty\": defaultdict(int),\n        \"with_code\": 0,\n        \"with_answer\": 0,\n        \"solution_lengths\": [],\n        \"problem_lengths\": [],\n    }\n    \n    for ex in examples:\n        stats[\"by_source\"][ex[\"source\"]] += 1\n        stats[\"by_difficulty\"][ex[\"difficulty\"]] += 1\n        if ex.get(\"has_code\"):\n            stats[\"with_code\"] += 1\n        if ex.get(\"answer\"):\n            stats[\"with_answer\"] += 1\n        stats[\"solution_lengths\"].append(len(ex.get(\"solution\", \"\")))\n        stats[\"problem_lengths\"].append(len(ex.get(\"problem\", \"\")))\n    \n    # Convert defaultdicts to regular dicts\n    stats[\"by_source\"] = dict(stats[\"by_source\"])\n    stats[\"by_difficulty\"] = dict(stats[\"by_difficulty\"])\n    \n    # Compute averages\n    stats[\"avg_solution_length\"] = sum(stats[\"solution_lengths\"]) / len(stats[\"solution_lengths\"]) if stats[\"solution_lengths\"] else 0\n    stats[\"avg_problem_length\"] = sum(stats[\"problem_lengths\"]) / len(stats[\"problem_lengths\"]) if stats[\"problem_lengths\"] else 0\n    stats[\"pct_with_code\"] = stats[\"with_code\"] / len(examples) if examples else 0\n    stats[\"pct_with_answer\"] = stats[\"with_answer\"] / len(examples) if examples else 0\n    \n    return stats\n\n\ndef log_to_wandb(sft_examples: List[Dict], grpo_examples: List[Dict], stats: Dict):\n    \"\"\"Log statistics and samples to Weights & Biases.\"\"\"\n    print(\"\\nüìä Logging to Weights & Biases...\")\n    \n    # Initialize W&B\n    run = wandb.init(\n        project=CONFIG[\"wandb_project\"],\n        name=CONFIG[\"wandb_run_name\"],\n        config=CONFIG\n    )\n    \n    # Log summary metrics\n    wandb.log({\n        \"sft_total_examples\": stats[\"total_examples\"],\n        \"grpo_total_examples\": len(grpo_examples),\n        \"pct_with_code\": stats[\"pct_with_code\"],\n        \"pct_with_answer\": stats[\"pct_with_answer\"],\n        \"avg_solution_length\": stats[\"avg_solution_length\"],\n        \"avg_problem_length\": stats[\"avg_problem_length\"],\n    })\n    \n    # Log distribution charts\n    # By source\n    source_data = [[k, v] for k, v in stats[\"by_source\"].items()]\n    wandb.log({\n        \"source_distribution\": wandb.plot.bar(\n            wandb.Table(data=source_data, columns=[\"source\", \"count\"]),\n            \"source\", \"count\", title=\"Examples by Source\"\n        )\n    })\n    \n    # By difficulty\n    diff_data = [[k, v] for k, v in stats[\"by_difficulty\"].items()]\n    wandb.log({\n        \"difficulty_distribution\": wandb.plot.bar(\n            wandb.Table(data=diff_data, columns=[\"difficulty\", \"count\"]),\n            \"difficulty\", \"count\", title=\"Examples by Difficulty\"\n        )\n    })\n    \n    # Solution length histogram\n    wandb.log({\n        \"solution_length_histogram\": wandb.Histogram(stats[\"solution_lengths\"])\n    })\n    \n    # Log sample examples as table\n    sample_size = min(100, len(sft_examples))\n    sample_data = []\n    for ex in random.sample(sft_examples, sample_size):\n        sample_data.append([\n            ex[\"source\"],\n            ex[\"difficulty\"],\n            ex[\"problem\"][:500] + \"...\" if len(ex[\"problem\"]) > 500 else ex[\"problem\"],\n            ex[\"solution\"][:500] + \"...\" if len(ex[\"solution\"]) > 500 else ex[\"solution\"],\n            ex[\"answer\"] or \"N/A\",\n            ex[\"has_code\"]\n        ])\n    \n    sample_table = wandb.Table(\n        data=sample_data,\n        columns=[\"source\", \"difficulty\", \"problem\", \"solution\", \"answer\", \"has_code\"]\n    )\n    wandb.log({\"sample_examples\": sample_table})\n    \n    # Log datasets as artifacts\n    sft_artifact = wandb.Artifact(\n        name=\"sft_dataset\",\n        type=\"dataset\",\n        description=f\"SFT dataset with {len(sft_examples)} examples\"\n    )\n    sft_artifact.add_file(Path(CONFIG[\"output_dir\"]) / CONFIG[\"sft_output\"])\n    run.log_artifact(sft_artifact)\n    \n    grpo_artifact = wandb.Artifact(\n        name=\"grpo_dataset\", \n        type=\"dataset\",\n        description=f\"GRPO dataset with {len(grpo_examples)} AIME problems\"\n    )\n    grpo_artifact.add_file(Path(CONFIG[\"output_dir\"]) / CONFIG[\"grpo_output\"])\n    run.log_artifact(grpo_artifact)\n    \n    wandb.finish()\n    print(\"   ‚úÖ Logged to W&B\")\n\n\ndef save_datasets(sft_examples: List[Dict], grpo_examples: List[Dict]):\n    \"\"\"Save datasets to JSONL files.\"\"\"\n    output_dir = Path(CONFIG[\"output_dir\"])\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Save SFT dataset\n    sft_path = output_dir / CONFIG[\"sft_output\"]\n    print(f\"\\nüíæ Saving SFT dataset to {sft_path}...\")\n    with open(sft_path, 'w', encoding='utf-8') as f:\n        for ex in tqdm(sft_examples, desc=\"   Writing\"):\n            f.write(json.dumps(ex, ensure_ascii=False) + '\\n')\n    \n    # Save GRPO dataset\n    grpo_path = output_dir / CONFIG[\"grpo_output\"]\n    print(f\"üíæ Saving GRPO dataset to {grpo_path}...\")\n    with open(grpo_path, 'w', encoding='utf-8') as f:\n        for ex in grpo_examples:\n            f.write(json.dumps(ex, ensure_ascii=False) + '\\n')\n    \n    print(f\"   ‚úÖ Saved {len(sft_examples)} SFT examples\")\n    print(f\"   ‚úÖ Saved {len(grpo_examples)} GRPO examples\")\n\n\ndef main():\n    \"\"\"Main function to prepare all datasets.\"\"\"\n    print(\"=\" * 60)\n    print(\"üöÄ Phi-4 Math Dataset Preparation\")\n    print(\"=\" * 60)\n    \n    set_seed(CONFIG[\"seed\"])\n    \n    # Download datasets\n    tir_examples = download_numina_tir()\n    cot_examples = download_numina_cot(CONFIG[\"numina_cot_sample\"])\n    aime_examples, grpo_examples = download_aime(CONFIG[\"aime_min_year\"])\n    \n    # Merge all examples\n    print(\"\\nüîÄ Merging datasets...\")\n    all_examples = tir_examples + cot_examples + aime_examples\n    print(f\"   Total before balancing: {len(all_examples)}\")\n    \n    # Balance by difficulty\n    balanced_examples = balance_by_difficulty(\n        all_examples,\n        CONFIG[\"difficulty_split\"]\n    )\n    \n    # Compute statistics\n    stats = compute_statistics(balanced_examples)\n    \n    # Print summary\n    print(\"\\n\" + \"=\" * 60)\n    print(\"üìä DATASET SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"   SFT Examples:  {stats['total_examples']:,}\")\n    print(f\"   GRPO Examples: {len(grpo_examples):,}\")\n    print(f\"   With Code:     {stats['pct_with_code']:.1%}\")\n    print(f\"   With Answer:   {stats['pct_with_answer']:.1%}\")\n    print(f\"   Avg Solution:  {stats['avg_solution_length']:.0f} chars\")\n    print(\"\\n   By Source:\")\n    for source, count in stats[\"by_source\"].items():\n        print(f\"      {source}: {count:,}\")\n    print(\"\\n   By Difficulty:\")\n    for diff, count in stats[\"by_difficulty\"].items():\n        pct = count / stats[\"total_examples\"] * 100\n        print(f\"      {diff}: {count:,} ({pct:.1f}%)\")\n    \n    # Save datasets\n    save_datasets(balanced_examples, grpo_examples)\n    \n    # Log to W&B\n    try:\n        log_to_wandb(balanced_examples, grpo_examples, stats)\n    except Exception as e:\n        print(f\"\\n‚ö†Ô∏è W&B logging failed: {e}\")\n        print(\"   Datasets saved locally. Run 'wandb login' to enable logging.\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚úÖ Dataset preparation complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:00:38.382786Z","iopub.execute_input":"2025-11-30T11:00:38.383045Z","iopub.status.idle":"2025-11-30T11:01:29.753220Z","shell.execute_reply.started":"2025-11-30T11:00:38.383008Z","shell.execute_reply":"2025-11-30T11:01:29.752730Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nüöÄ Phi-4 Math Dataset Preparation\n============================================================\n\nüì• Downloading NuminaMath-TIR...\n","output_type":"stream"},{"name":"stderr","text":"   Processing TIR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72441/72441 [00:02<00:00, 24961.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Loaded 72441 examples from NuminaMath-TIR\n\nüì• Downloading NuminaMath-CoT (sampling 40000)...\n","output_type":"stream"},{"name":"stderr","text":"   Processing CoT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 859494/859494 [00:40<00:00, 21462.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Loaded 40000 examples from NuminaMath-CoT (from 859494 total)\n\nüì• Downloading AIME (year >= 2000)...\n   Columns: ['ID', 'Year', 'Problem Number', 'Question', 'Answer', 'Part']\n","output_type":"stream"},{"name":"stderr","text":"   Processing AIME: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 32095.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Loaded 709 AIME GRPO examples (year >= 2000)\n   ‚ö†Ô∏è AIME has no solutions, only used for GRPO\n\nüîÄ Merging datasets...\n   Total before balancing: 112441\n\n‚öñÔ∏è Balancing difficulty distribution...\n   Current distribution: {'hard': '48.3%', 'medium': '46.3%', 'easy': '5.4%'}\n   Target distribution:  {'easy': '20.0%', 'medium': '30.0%', 'hard': '50.0%'}\n   Final distribution: {'hard': '50.0% (15187)', 'easy': '20.0% (6075)', 'medium': '30.0% (9112)'}\n   Final dataset size: 30374\n\n============================================================\nüìä DATASET SUMMARY\n============================================================\n   SFT Examples:  30,374\n   GRPO Examples: 709\n   With Code:     57.6%\n   With Answer:   97.7%\n   Avg Solution:  1642 chars\n\n   By Source:\n      numina_tir: 17,511\n      numina_cot: 12,863\n\n   By Difficulty:\n      hard: 15,187 (50.0%)\n      easy: 6,075 (20.0%)\n      medium: 9,112 (30.0%)\n\nüíæ Saving SFT dataset to data/sft_dataset.jsonl...\n","output_type":"stream"},{"name":"stderr","text":"   Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30374/30374 [00:01<00:00, 24459.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"üíæ Saving GRPO dataset to data/grpo_dataset.jsonl...\n   ‚úÖ Saved 30374 SFT examples\n   ‚úÖ Saved 709 GRPO examples\n\nüìä Logging to Weights & Biases...\n\n‚ö†Ô∏è W&B logging failed: Error uploading run: returned error 403: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertBucket\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}]}\n   Datasets saved locally. Run 'wandb login' to enable logging.\n\n============================================================\n‚úÖ Dataset preparation complete!\n============================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\naime = load_dataset(\"di-zhang-fdu/AIME_1983_2024\", split=\"train\")\nprint(\"Columns:\", aime.column_names)\nprint(\"\\nSample row:\")\nprint(aime[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:56:30.474359Z","iopub.execute_input":"2025-11-30T10:56:30.474964Z","iopub.status.idle":"2025-11-30T10:56:30.716401Z","shell.execute_reply.started":"2025-11-30T10:56:30.474948Z","shell.execute_reply":"2025-11-30T10:56:30.715965Z"}},"outputs":[{"name":"stdout","text":"Columns: ['ID', 'Year', 'Problem Number', 'Question', 'Answer', 'Part']\n\nSample row:\n{'ID': '1983-1', 'Year': 1983, 'Problem Number': 1, 'Question': 'Let $x$ , $y$ and $z$ all exceed $1$ and let $w$ be a positive number such that $\\\\log_xw=24$ , $\\\\log_y w = 40$ and $\\\\log_{xyz}w=12$ . Find $\\\\log_zw$ .', 'Answer': '60', 'Part': None}\n","output_type":"stream"}],"execution_count":2}]}