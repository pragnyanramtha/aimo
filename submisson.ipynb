{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":283145406,"sourceType":"kernelVersion"},{"sourceId":283146302,"sourceType":"kernelVersion"},{"sourceId":283148209,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport time\nimport re\nimport tempfile\nimport subprocess\nfrom collections import Counter, defaultdict\nfrom typing import Optional\n\nimport pandas as pd\nimport polars as pl\nfrom transformers import set_seed\nimport torch\nfrom vllm import LLM, SamplingParams\nimport kaggle_evaluation.aimo_3_inference_server\n\nset_seed(42)\npd.set_option('display.max_colwidth', None)\ncutoff_time = time.time() + (4 * 60 + 45) * 60\n\n\nimport os\nimport shutil\n\n# Use /kaggle/tmp - no 20GB limit!\nmodel_dir = \"/kaggle/tmp/phi4-reasoning-plus\"\nos.makedirs(model_dir, exist_ok=True)\n\n# Copy from both parts (actual copy is fine here)\nfor part in [\"phi-4-reasoning-plus-1-2\", \"phi-4-reasoning-plus-2-2\"]:\n    part_dir = f\"/kaggle/input/{part}\"\n    if os.path.exists(part_dir):\n        for file in os.listdir(part_dir):\n            src = os.path.join(part_dir, file)\n            dst = os.path.join(model_dir, file)\n            if os.path.isfile(src) and not os.path.exists(dst):\n                shutil.copy2(src, dst)\n                print(f\"Copied: {file}\")\n\nprint(\"\\nModel files:\")\nfor f in sorted(os.listdir(model_dir)):\n    size_gb = os.path.getsize(os.path.join(model_dir, f)) / 1024**3\n    print(f\"  {f}: {size_gb:.2f} GB\")\n\n# Load from tmp\nLLM_MODEL_PATH = \"/kaggle/tmp/phi4-reasoning-plus\"\n\n# Load the model\nllm = LLM(\n    LLM_MODEL_PATH,\n    dtype=\"bfloat16\",\n    max_num_seqs=256,\n    max_model_len=32768,\n    trust_remote_code=True,\n    tensor_parallel_size=1,\n    gpu_memory_utilization=0.96,\n)\n\ntokenizer = llm.get_tokenizer()\n\nsampling_params = SamplingParams(\n    temperature=0.8,\n    min_p=0.01,\n    skip_special_tokens=True,\n    max_tokens=32768,\n)\n\n# Microsoft's recommended system prompt for Phi-4-Reasoning-Plus\nSYSTEM_PROMPT = \"\"\"You have the role of a math professor exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> {Thought section} </think> {Solution section}. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Put your final numerical answer in \\\\boxed{}.\"\"\"\n\n# Multiple prompts for diversity\nthoughts = [\n    SYSTEM_PROMPT,\n    SYSTEM_PROMPT + \" Double-check your arithmetic carefully.\",\n    SYSTEM_PROMPT + \" Consider multiple approaches before settling on an answer.\",\n    SYSTEM_PROMPT + \" Verify your solution by substituting back.\",\n    SYSTEM_PROMPT + \" Break down the problem into smaller parts.\",\n]\n\ndef extract_boxed_answers(text: str) -> list[int]:\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return []\n    ans = []\n    for content in matches:\n        content = content.replace(',', '').replace(' ', '')\n        if content.isdigit():\n            num = content\n        else:\n            nums = re.findall(r'\\d+', content)\n            if not nums:\n                continue\n            num = nums[-1]\n        try:\n            ans.append(int(num))\n        except:\n            pass\n    return ans\n\ndef select_answer(answers: list):\n    valid_answers = []\n    for answer in answers:\n        try:\n            if int(answer) != float(answer):\n                continue\n            if 0 <= int(answer) <= 99999:\n                valid_answers.append(int(answer))\n        except:\n            pass\n    if not valid_answers:\n        return 49\n    answer, _ = Counter(valid_answers).most_common(1)[0]\n    return answer\n\ndef extract_python_code(text: str) -> list[str]:\n    pattern = r'```python\\s*(.*?)\\s*```'\n    matches = re.findall(pattern, text, re.DOTALL)\n    return matches\n\ndef process_python_code(query):\n    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\nfrom sympy import *\\n\" + query\n    return query.strip()\n\nclass PythonREPL:\n    def __init__(self, timeout=8):\n        self.timeout = timeout\n\n    def __call__(self, query):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(query)\n            try:\n                result = subprocess.run(\n                    [\"python3\", temp_file_path],\n                    capture_output=True,\n                    check=False,\n                    text=True,\n                    timeout=self.timeout,\n                )\n            except subprocess.TimeoutExpired:\n                return False, f\"Timeout after {self.timeout}s\"\n            if result.returncode == 0:\n                return True, result.stdout.strip()\n            return False, result.stderr.strip()\n\nMessagesBatch = list[list[dict[str, str]]]\nanswer_contributions = defaultdict(list)\n\ndef format_phi4_prompt(system: str, user: str) -> str:\n    \"\"\"Format prompt using Phi-4's chat template\"\"\"\n    return f\"<|im_start|>system<|im_sep|>\\n{system}<|im_end|>\\n<|im_start|>user<|im_sep|>\\n{user}<|im_end|>\\n<|im_start|>assistant<|im_sep|>\\n\"\n\ndef batch_message_generate(msg_batch: MessagesBatch) -> MessagesBatch:\n    list_of_texts = []\n    for messages in msg_batch:\n        system_msg = messages[0]['content']\n        user_msg = messages[1]['content']\n        prompt = format_phi4_prompt(system_msg, user_msg)\n        list_of_texts.append(prompt)\n    \n    request_output = llm.generate(prompts=list_of_texts, sampling_params=sampling_params)\n    for messages, single_request_output in zip(msg_batch, request_output):\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n    return msg_batch\n\ndef batch_message_filter(msg_batch: MessagesBatch, list_of_idx: list[int]):\n    global answer_contributions\n    extracted_answers = []\n    msgs_to_keep = []\n    idx_to_keep = []\n    for idx, messages in zip(list_of_idx, msg_batch):\n        answers = extract_boxed_answers(messages[-1]['content'])\n        if answers:\n            extracted_answers.extend(answers)\n            for answer in answers:\n                answer_contributions[answer].append(idx)\n        else:\n            msgs_to_keep.append(messages)\n            idx_to_keep.append(idx)\n    return msgs_to_keep, extracted_answers, idx_to_keep\n\ndef batch_execute_and_get_answer(list_of_messages: MessagesBatch) -> list[int]:\n    ans = []\n    for messages in list_of_messages:\n        python_code_list = extract_python_code(messages[-1]['content'])\n        for python_code in python_code_list:\n            python_code = process_python_code(python_code)\n            try:\n                success, output = PythonREPL()(python_code)\n                if not success:\n                    continue\n                matches = re.findall(r'(\\d+)', output)\n                for match in matches:\n                    ans.append(int(match))\n            except:\n                pass\n    return ans\n\ndef predict_for_question(question: str, max_rounds: int = 1) -> int:\n    global answer_contributions\n    if time.time() > cutoff_time:\n        return 210\n    \n    msgs_batch: MessagesBatch = [\n        [{\"role\": \"system\", \"content\": t}, {\"role\": \"user\", \"content\": question}]\n        for t in thoughts\n    ]\n    \n    all_extracted_answers = []\n    list_of_idx = list(range(len(msgs_batch)))\n    \n    for round_idx in range(max_rounds):\n        msgs_batch = batch_message_generate(msgs_batch)\n        extracted_python_answer = batch_execute_and_get_answer(msgs_batch)\n        msgs_batch, extracted_answers, list_of_idx = batch_message_filter(msgs_batch, list_of_idx)\n        all_extracted_answers.extend(extracted_python_answer)\n        all_extracted_answers.extend(extracted_answers)\n        if not msgs_batch:\n            break\n    \n    return select_answer(all_extracted_answers)\n\ndef predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n    id_ = id_.item(0)\n    question_str = question.item(0)\n    answer = predict_for_question(question_str)\n    return pl.DataFrame({'id': id_, 'answer': answer})\n\n# Start inference server\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:21:02.835733Z","iopub.execute_input":"2025-12-01T15:21:02.836225Z","iopub.status.idle":"2025-12-01T15:30:24.639702Z","shell.execute_reply.started":"2025-12-01T15:21:02.836208Z","shell.execute_reply":"2025-12-01T15:30:24.639167Z"}},"outputs":[],"execution_count":null}]}