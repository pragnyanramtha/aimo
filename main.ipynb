{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":13931380,"sourceType":"datasetVersion","datasetId":8877966}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pragnyanramtha/ai-math?scriptVersionId=282869104\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"! pip install uv\n! uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n! uv pip install -U peft datasets pandas scikit-learn accelerate numpy==1.26.4 scikit-learn transformers trl bitsandbytes unsloth unsloth_zoo Pillow\n! uv pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3+cu128torch2.9-cp311-cp311-linux_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:21:01.935647Z","iopub.execute_input":"2025-11-30T13:21:01.935779Z","iopub.status.idle":"2025-11-30T13:21:05.072556Z","shell.execute_reply.started":"2025-11-30T13:21:01.935767Z","shell.execute_reply":"2025-11-30T13:21:05.071999Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.9.13)\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m42 packages\u001b[0m \u001b[2min 219ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 35ms\u001b[0m\u001b[0m                                               \n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0mu128                              \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.9.1+cu128\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.9.0+cu128\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m107 packages\u001b[0m \u001b[2min 107ms\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2mAudited \u001b[1m107 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 76ms\u001b[0m\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nSFT Training Script for Phi-4-Reasoning-Plus\nUnsloth + Maximum Quality Settings\n\"\"\"\n\nimport os\nimport time\nimport torch\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nfrom unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\n\nfrom transformers import TrainingArguments, TrainerCallback\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\nclass RealTimeLogger(TrainerCallback):\n    def __init__(self):\n        self.start_time = None\n        \n    def on_train_begin(self, args, state, control, **kwargs):\n        self.start_time = time.time()\n        print(\"\\n\" + \"=\"*70)\n        print(\"üöÄ TRAINING STARTED (Unsloth + LoRA r=512 + Max Quality)\")\n        print(\"=\"*70 + \"\\n\")\n        \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and \"loss\" in logs:\n            step = state.global_step\n            total = state.max_steps\n            pct = (step / total) * 100 if total > 0 else 0\n            loss = logs.get(\"loss\", 0)\n            lr = logs.get(\"learning_rate\", 0)\n            epoch = logs.get(\"epoch\", 0)\n            \n            elapsed = time.time() - self.start_time\n            steps_per_sec = step / elapsed if elapsed > 0 else 0\n            remaining = (total - step) / steps_per_sec if steps_per_sec > 0 else 0\n            \n            if torch.cuda.is_available():\n                mem_used = torch.cuda.memory_allocated() / 1e9\n                mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n            else:\n                mem_used, mem_total = 0, 1\n            \n            print(f\"[{pct:5.1f}%] Step {step:>5}/{total} | \"\n                  f\"Loss: {loss:.4f} | LR: {lr:.2e} | \"\n                  f\"Epoch: {epoch:.2f} | VRAM: {mem_used:.0f}/{mem_total:.0f}GB | \"\n                  f\"ETA: {timedelta(seconds=int(remaining))}\")\n    \n    def on_save(self, args, state, control, **kwargs):\n        print(f\"\\nüíæ Checkpoint saved at step {state.global_step}\\n\")\n    \n    def on_train_end(self, args, state, control, **kwargs):\n        elapsed = time.time() - self.start_time\n        print(\"\\n\" + \"=\"*70)\n        print(f\"‚úÖ TRAINING COMPLETE | Time: {timedelta(seconds=int(elapsed))}\")\n        print(\"=\"*70 + \"\\n\")\n\n\n# === MAXIMUM QUALITY Configuration ===\nCONFIG = {\n    \"model_name\": \"microsoft/Phi-4-reasoning-plus\",\n    \"max_seq_length\": 8192,  # Increased from 4096\n    \"dataset_path\": \"/kaggle/input/aimath-train/data/sft_dataset.jsonl\",\n    \"output_dir\": \"/kaggle/working/outputs/sft\",\n    \n    # LoRA - MAXIMUM RANK\n    \"lora_r\": 512,           # Increased from 256\n    \"lora_alpha\": 512,       # Match rank\n    \"lora_dropout\": 0,\n    \n    # Training - LARGER BATCHES\n    \"num_train_epochs\": 3,   # Increased from 2\n    \"per_device_train_batch_size\": 8,   # Increased from 2\n    \"gradient_accumulation_steps\": 4,   # Effective batch = 16\n    \"learning_rate\": 1e-4,   # Slightly lower for stability\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \n    # Saving\n    \"save_steps\": 200,\n    \"save_total_limit\": 2,\n    \"logging_steps\": 10,\n    \"seed\": 42,\n}\n\n\ndef load_model_and_tokenizer():\n    print(\"\\n\" + \"=\"*70)\n    print(\"üì• Loading Model with Unsloth (MAX QUALITY)\")\n    print(\"=\"*70)\n    \n    print(f\"\\n   Model: {CONFIG['model_name']}\")\n    print(f\"   LoRA rank: {CONFIG['lora_r']} (MAXIMUM)\")\n    print(f\"   Max seq length: {CONFIG['max_seq_length']}\")\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=CONFIG[\"model_name\"],\n        max_seq_length=CONFIG[\"max_seq_length\"],\n        dtype=torch.bfloat16,\n        load_in_4bit=False,\n        trust_remote_code=True,\n    )\n    \n    print(\"   ‚úÖ Base model loaded\")\n    \n    # Add MAXIMUM rank LoRA\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=CONFIG[\"lora_r\"],\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"lm_head\",      # Also train output layer\n            \"embed_tokens\", # Also train embeddings\n        ],\n        lora_alpha=CONFIG[\"lora_alpha\"],\n        lora_dropout=CONFIG[\"lora_dropout\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=CONFIG[\"seed\"],\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"   ‚úÖ LoRA applied (r={CONFIG['lora_r']})\")\n    print(f\"   üìä Total params: {total_params / 1e9:.2f}B\")\n    print(f\"   üìä Trainable params: {trainable_params / 1e6:.0f}M ({100*trainable_params/total_params:.1f}%)\")\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    if torch.cuda.is_available():\n        mem = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"   üìä VRAM: {mem:.1f} / {total:.0f} GB\")\n    \n    return model, tokenizer\n\n\ndef main():\n    start_time = time.time()\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üéØ Phi-4 Math SFT Training (MAXIMUM QUALITY)\")\n    print(\"=\"*70)\n    print(f\"   Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"   Platform: Kaggle H100 80GB\")\n    print(f\"   Method: Unsloth + LoRA r={CONFIG['lora_r']} + 3 epochs\")\n    \n    model, tokenizer = load_model_and_tokenizer()\n    \n    print(f\"\\nüìÇ Loading dataset...\")\n    dataset = load_dataset(\"json\", data_files=CONFIG[\"dataset_path\"], split=\"train\")\n    print(f\"   ‚úÖ Loaded {len(dataset):,} examples\")\n    \n    output_dir = Path(CONFIG[\"output_dir\"])\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Resume checkpoint\n    resume_checkpoint = None\n    checkpoints = list(output_dir.glob(\"checkpoint-*\"))\n    if checkpoints:\n        resume_checkpoint = str(max(checkpoints, key=lambda x: int(x.name.split(\"-\")[1])))\n        print(f\"\\nüîÑ Found checkpoint: {resume_checkpoint}\")\n    \n    training_args = TrainingArguments(\n        output_dir=str(output_dir),\n        num_train_epochs=CONFIG[\"num_train_epochs\"],\n        per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n        learning_rate=CONFIG[\"learning_rate\"],\n        lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n        warmup_ratio=CONFIG[\"warmup_ratio\"],\n        weight_decay=CONFIG[\"weight_decay\"],\n        max_grad_norm=CONFIG[\"max_grad_norm\"],\n        bf16=is_bfloat16_supported(),\n        fp16=not is_bfloat16_supported(),\n        optim=\"adamw_torch_fused\",\n        save_strategy=\"steps\",\n        save_steps=CONFIG[\"save_steps\"],\n        save_total_limit=CONFIG[\"save_total_limit\"],\n        logging_steps=CONFIG[\"logging_steps\"],\n        logging_first_step=True,\n        report_to=\"none\",\n        seed=CONFIG[\"seed\"],\n        dataloader_num_workers=2,\n    )\n    \n    effective_batch = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n    steps_per_epoch = len(dataset) // effective_batch\n    total_steps = steps_per_epoch * CONFIG[\"num_train_epochs\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üìã Training Configuration (MAXIMUM QUALITY)\")\n    print(\"=\"*70)\n    print(f\"   Dataset:         {len(dataset):,} examples\")\n    print(f\"   Max seq length:  {CONFIG['max_seq_length']}\")\n    print(f\"   LoRA rank:       {CONFIG['lora_r']} (+ embed + lm_head)\")\n    print(f\"   Batch size:      {CONFIG['per_device_train_batch_size']} x {CONFIG['gradient_accumulation_steps']} = {effective_batch}\")\n    print(f\"   Epochs:          {CONFIG['num_train_epochs']}\")\n    print(f\"   Learning rate:   {CONFIG['learning_rate']}\")\n    print(f\"   Steps/epoch:     {steps_per_epoch:,}\")\n    print(f\"   Total steps:     {total_steps:,}\")\n    print(f\"   Est. time:       {timedelta(seconds=total_steps * 2)}\")  # ~2s/step with larger batch\n    print(\"=\"*70)\n    \n    print(\"\\nüèãÔ∏è Creating SFT Trainer...\")\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset,\n        args=training_args,\n        dataset_text_field=\"text\",\n        max_seq_length=CONFIG[\"max_seq_length\"],\n        packing=True,\n        callbacks=[RealTimeLogger()],\n    )\n    \n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        mem = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"\\nüìä VRAM before training: {mem:.1f} / {total:.0f} GB\")\n    \n    if resume_checkpoint:\n        print(f\"\\nüîÑ Resuming from {resume_checkpoint}\")\n        trainer.train(resume_from_checkpoint=resume_checkpoint)\n    else:\n        trainer.train()\n    \n    # Save\n    print(\"\\nüíæ Saving LoRA model...\")\n    lora_path = output_dir / \"lora_model\"\n    model.save_pretrained(str(lora_path))\n    tokenizer.save_pretrained(str(lora_path))\n    print(f\"   ‚úÖ LoRA adapters saved to: {lora_path}\")\n    \n    print(\"\\nüíæ Merging LoRA into base model (16-bit)...\")\n    merged_path = output_dir / \"merged_model\"\n    model.save_pretrained_merged(\n        str(merged_path),\n        tokenizer,\n        save_method=\"merged_16bit\",\n    )\n    print(f\"   ‚úÖ Merged model saved to: {merged_path}\")\n    \n    elapsed = time.time() - start_time\n    print(f\"\\n‚úÖ Training complete!\")\n    print(f\"   Total time: {timedelta(seconds=int(elapsed))}\")\n    \n    return str(merged_path)\n\n\nif __name__ == \"__main__\":\n    final_model_path = main()\n    print(f\"\\nüéâ SFT Complete! Model at: {final_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:28:30.271153Z","iopub.execute_input":"2025-11-30T13:28:30.271762Z","iopub.status.idle":"2025-11-30T13:32:28.244043Z","shell.execute_reply.started":"2025-11-30T13:28:30.271744Z","shell.execute_reply":"2025-11-30T13:32:28.243349Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüéØ Phi-4 Math SFT Training (MAXIMUM QUALITY)\n======================================================================\n   Started: 2025-11-30 13:28:30\n   Platform: Kaggle H100 80GB\n   Method: Unsloth + LoRA r=512 + 3 epochs\n\n======================================================================\nüì• Loading Model with Unsloth (MAX QUALITY)\n======================================================================\n\n   Model: microsoft/Phi-4-reasoning-plus\n   LoRA rank: 512 (MAXIMUM)\n   Max seq length: 8192\nUnsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.11.4: Fast Phi3 patching. Transformers: 4.57.2.\n   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.437 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Phi3 does not support SDPA - switching to fast eager.\nUnsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87561bcb3d3d4ab9b3d3b005f2c5c2a1"}},"metadata":{}},{"name":"stdout","text":"   ‚úÖ Base model loaded\nUnsloth: Making `model.base_model.model.model.embed_tokens` require gradients\n   ‚úÖ LoRA applied (r=512)\n   üìä Total params: 15.45B\n   üìä Trainable params: 736M (4.8%)\n   üìä VRAM: 33.7 / 85 GB\n\nüìÇ Loading dataset...\n   ‚úÖ Loaded 30,374 examples\n\n======================================================================\nüìã Training Configuration (MAXIMUM QUALITY)\n======================================================================\n   Dataset:         30,374 examples\n   Max seq length:  8192\n   LoRA rank:       512 (+ embed + lm_head)\n   Batch size:      4 x 4 = 16\n   Epochs:          3\n   Learning rate:   0.0001\n   Steps/epoch:     1,898\n   Total steps:     5,694\n   Est. time:       3:09:48\n======================================================================\n\nüèãÔ∏è Creating SFT Trainer...\n","output_type":"stream"},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"\nüìä VRAM before training: 33.7 / 85 GB\n\n======================================================================\nüöÄ TRAINING STARTED (Unsloth + LoRA r=512 + Max Quality)\n======================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='58' max='5697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  58/5697 03:28 < 5:50:12, 0.27 it/s, Epoch 0.03/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.782300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.817900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.679100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.572900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.486300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.462700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[  0.0%] Step     1/5697 | Loss: 0.7823 | LR: 0.00e+00 | Epoch: 0.00 | VRAM: 35/85GB | ETA: 7:18:36\n[  0.2%] Step    10/5697 | Loss: 0.8179 | LR: 5.26e-06 | Epoch: 0.01 | VRAM: 35/85GB | ETA: 6:16:37\n[  0.4%] Step    20/5697 | Loss: 0.6791 | LR: 1.11e-05 | Epoch: 0.01 | VRAM: 35/85GB | ETA: 6:07:55\n[  0.5%] Step    30/5697 | Loss: 0.5729 | LR: 1.70e-05 | Epoch: 0.02 | VRAM: 35/85GB | ETA: 6:02:50\n[  0.7%] Step    40/5697 | Loss: 0.4863 | LR: 2.28e-05 | Epoch: 0.02 | VRAM: 35/85GB | ETA: 5:54:49\n[  0.9%] Step    50/5697 | Loss: 0.4627 | LR: 2.87e-05 | Epoch: 0.03 | VRAM: 35/85GB | ETA: 5:54:41\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_299/939623204.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfinal_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüéâ SFT Complete! Model at: {final_model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_299/939623204.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5}]}