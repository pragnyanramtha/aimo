{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":13931380,"sourceType":"datasetVersion","datasetId":8877966}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pragnyanramtha/ai-math?scriptVersionId=282874042\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"\"\"\"\nSFT Training Script for Phi-4-Reasoning-Plus\nUnsloth + Maximum Quality Settings\n\"\"\"\n\nimport os\nimport time\nimport torch\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nfrom unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\n\nfrom transformers import TrainingArguments, TrainerCallback\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\nclass RealTimeLogger(TrainerCallback):\n    def __init__(self):\n        self.start_time = None\n        \n    def on_train_begin(self, args, state, control, **kwargs):\n        self.start_time = time.time()\n        print(\"\\n\" + \"=\"*70)\n        print(\"üöÄ TRAINING STARTED (Unsloth + LoRA r=512 + Max Quality)\")\n        print(\"=\"*70 + \"\\n\")\n        \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and \"loss\" in logs:\n            step = state.global_step\n            total = state.max_steps\n            pct = (step / total) * 100 if total > 0 else 0\n            loss = logs.get(\"loss\", 0)\n            lr = logs.get(\"learning_rate\", 0)\n            epoch = logs.get(\"epoch\", 0)\n            \n            elapsed = time.time() - self.start_time\n            steps_per_sec = step / elapsed if elapsed > 0 else 0\n            remaining = (total - step) / steps_per_sec if steps_per_sec > 0 else 0\n            \n            if torch.cuda.is_available():\n                mem_used = torch.cuda.memory_allocated() / 1e9\n                mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n            else:\n                mem_used, mem_total = 0, 1\n            \n            print(f\"[{pct:5.1f}%] Step {step:>5}/{total} | \"\n                  f\"Loss: {loss:.4f} | LR: {lr:.2e} | \"\n                  f\"Epoch: {epoch:.2f} | VRAM: {mem_used:.0f}/{mem_total:.0f}GB | \"\n                  f\"ETA: {timedelta(seconds=int(remaining))}\")\n    \n    def on_save(self, args, state, control, **kwargs):\n        print(f\"\\nüíæ Checkpoint saved at step {state.global_step}\\n\")\n    \n    def on_train_end(self, args, state, control, **kwargs):\n        elapsed = time.time() - self.start_time\n        print(\"\\n\" + \"=\"*70)\n        print(f\"‚úÖ TRAINING COMPLETE | Time: {timedelta(seconds=int(elapsed))}\")\n        print(\"=\"*70 + \"\\n\")\n\n\n# === MAXIMUM QUALITY Configuration ===\nCONFIG = {\n    \"model_name\": \"microsoft/Phi-4-reasoning-plus\",\n    \"max_seq_length\": 8192,  # Increased from 4096\n    \"dataset_path\": \"/kaggle/input/aimath-train/data/sft_dataset.jsonl\",\n    \"output_dir\": \"/kaggle/working/outputs/sft\",\n    \n    # LoRA - MAXIMUM RANK\n    \"lora_r\": 512,           # Increased from 256\n    \"lora_alpha\": 512,       # Match rank\n    \"lora_dropout\": 0,\n    \n    # Training - LARGER BATCHES\n    \"num_train_epochs\": 3,   # Increased from 2\n    \"per_device_train_batch_size\": 8,   # Increased from 2\n    \"gradient_accumulation_steps\": 4,   # Effective batch = 16\n    \"learning_rate\": 1e-4,   # Slightly lower for stability\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \n    # Saving\n    \"save_steps\": 200,\n    \"save_total_limit\": 2,\n    \"logging_steps\": 10,\n    \"seed\": 42,\n}\n\n\ndef load_model_and_tokenizer():\n    print(\"\\n\" + \"=\"*70)\n    print(\"üì• Loading Model with Unsloth (MAX QUALITY)\")\n    print(\"=\"*70)\n    \n    print(f\"\\n   Model: {CONFIG['model_name']}\")\n    print(f\"   LoRA rank: {CONFIG['lora_r']} (MAXIMUM)\")\n    print(f\"   Max seq length: {CONFIG['max_seq_length']}\")\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=CONFIG[\"model_name\"],\n        max_seq_length=CONFIG[\"max_seq_length\"],\n        dtype=torch.bfloat16,\n        load_in_4bit=False,\n        trust_remote_code=True,\n    )\n    \n    print(\"   ‚úÖ Base model loaded\")\n    \n    # Add MAXIMUM rank LoRA\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=CONFIG[\"lora_r\"],\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n            \"lm_head\",      # Also train output layer\n            \"embed_tokens\", # Also train embeddings\n        ],\n        lora_alpha=CONFIG[\"lora_alpha\"],\n        lora_dropout=CONFIG[\"lora_dropout\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=CONFIG[\"seed\"],\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"   ‚úÖ LoRA applied (r={CONFIG['lora_r']})\")\n    print(f\"   üìä Total params: {total_params / 1e9:.2f}B\")\n    print(f\"   üìä Trainable params: {trainable_params / 1e6:.0f}M ({100*trainable_params/total_params:.1f}%)\")\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    if torch.cuda.is_available():\n        mem = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"   üìä VRAM: {mem:.1f} / {total:.0f} GB\")\n    \n    return model, tokenizer\n\n\ndef main():\n    start_time = time.time()\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üéØ Phi-4 Math SFT Training (MAXIMUM QUALITY)\")\n    print(\"=\"*70)\n    print(f\"   Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"   Platform: Kaggle H100 80GB\")\n    print(f\"   Method: Unsloth + LoRA r={CONFIG['lora_r']} + 3 epochs\")\n    \n    model, tokenizer = load_model_and_tokenizer()\n    \n    print(f\"\\nüìÇ Loading dataset...\")\n    dataset = load_dataset(\"json\", data_files=CONFIG[\"dataset_path\"], split=\"train\")\n    print(f\"   ‚úÖ Loaded {len(dataset):,} examples\")\n    \n    output_dir = Path(CONFIG[\"output_dir\"])\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Resume checkpoint\n    resume_checkpoint = None\n    checkpoints = list(output_dir.glob(\"checkpoint-*\"))\n    if checkpoints:\n        resume_checkpoint = str(max(checkpoints, key=lambda x: int(x.name.split(\"-\")[1])))\n        print(f\"\\nüîÑ Found checkpoint: {resume_checkpoint}\")\n    \n    training_args = TrainingArguments(\n        output_dir=str(output_dir),\n        num_train_epochs=CONFIG[\"num_train_epochs\"],\n        per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n        learning_rate=CONFIG[\"learning_rate\"],\n        lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n        warmup_ratio=CONFIG[\"warmup_ratio\"],\n        weight_decay=CONFIG[\"weight_decay\"],\n        max_grad_norm=CONFIG[\"max_grad_norm\"],\n        bf16=is_bfloat16_supported(),\n        fp16=not is_bfloat16_supported(),\n        optim=\"adamw_torch_fused\",\n        save_strategy=\"steps\",\n        save_steps=CONFIG[\"save_steps\"],\n        save_total_limit=CONFIG[\"save_total_limit\"],\n        logging_steps=CONFIG[\"logging_steps\"],\n        logging_first_step=True,\n        report_to=\"none\",\n        seed=CONFIG[\"seed\"],\n        dataloader_num_workers=2,\n    )\n    \n    effective_batch = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n    steps_per_epoch = len(dataset) // effective_batch\n    total_steps = steps_per_epoch * CONFIG[\"num_train_epochs\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üìã Training Configuration (MAXIMUM QUALITY)\")\n    print(\"=\"*70)\n    print(f\"   Dataset:         {len(dataset):,} examples\")\n    print(f\"   Max seq length:  {CONFIG['max_seq_length']}\")\n    print(f\"   LoRA rank:       {CONFIG['lora_r']} (+ embed + lm_head)\")\n    print(f\"   Batch size:      {CONFIG['per_device_train_batch_size']} x {CONFIG['gradient_accumulation_steps']} = {effective_batch}\")\n    print(f\"   Epochs:          {CONFIG['num_train_epochs']}\")\n    print(f\"   Learning rate:   {CONFIG['learning_rate']}\")\n    print(f\"   Steps/epoch:     {steps_per_epoch:,}\")\n    print(f\"   Total steps:     {total_steps:,}\")\n    print(f\"   Est. time:       {timedelta(seconds=total_steps * 2)}\")  # ~2s/step with larger batch\n    print(\"=\"*70)\n    \n    print(\"\\nüèãÔ∏è Creating SFT Trainer...\")\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset,\n        args=training_args,\n        dataset_text_field=\"text\",\n        max_seq_length=CONFIG[\"max_seq_length\"],\n        packing=True,\n        callbacks=[RealTimeLogger()],\n    )\n    \n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        mem = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"\\nüìä VRAM before training: {mem:.1f} / {total:.0f} GB\")\n    \n    if resume_checkpoint:\n        print(f\"\\nüîÑ Resuming from {resume_checkpoint}\")\n        trainer.train(resume_from_checkpoint=resume_checkpoint)\n    else:\n        trainer.train()\n    \n    # Save\n    print(\"\\nüíæ Saving LoRA model...\")\n    lora_path = output_dir / \"lora_model\"\n    model.save_pretrained(str(lora_path))\n    tokenizer.save_pretrained(str(lora_path))\n    print(f\"   ‚úÖ LoRA adapters saved to: {lora_path}\")\n    \n    print(\"\\nüíæ Merging LoRA into base model (16-bit)...\")\n    merged_path = output_dir / \"merged_model\"\n    model.save_pretrained_merged(\n        str(merged_path),\n        tokenizer,\n        save_method=\"merged_16bit\",\n    )\n    print(f\"   ‚úÖ Merged model saved to: {merged_path}\")\n    \n    elapsed = time.time() - start_time\n    print(f\"\\n‚úÖ Training complete!\")\n    print(f\"   Total time: {timedelta(seconds=int(elapsed))}\")\n    \n    return str(merged_path)\n\n\nif __name__ == \"__main__\":\n    final_model_path = main()\n    print(f\"\\nüéâ SFT Complete! Model at: {final_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:28:30.271153Z","iopub.execute_input":"2025-11-30T13:28:30.271762Z","iopub.status.idle":"2025-11-30T13:32:28.244043Z","shell.execute_reply.started":"2025-11-30T13:28:30.271744Z","shell.execute_reply":"2025-11-30T13:32:28.243349Z"}},"outputs":[],"execution_count":null}]}