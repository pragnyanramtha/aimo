{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":13931380,"sourceType":"datasetVersion","datasetId":8877966}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pragnyanramtha/ai-math?scriptVersionId=282855868\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"! pip install uv\n! uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n! uv pip install -U peft datasets deepspeed scikit-learn accelerate numpy==1.26.4 scikit-learn transformers trl\n! uv pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3+cu128torch2.9-cp311-cp311-linux_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:34:08.793044Z","iopub.execute_input":"2025-11-30T12:34:08.793208Z","iopub.status.idle":"2025-11-30T12:34:13.290887Z","shell.execute_reply.started":"2025-11-30T12:34:08.793196Z","shell.execute_reply":"2025-11-30T12:34:13.290367Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.9.13)\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 73ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m80 packages\u001b[0m \u001b[2min 1.79s\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2mAudited \u001b[1m80 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 77ms\u001b[0m\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nSFT Training Script for Phi-4-Reasoning-Plus\nFull fine-tuning with DeepSpeed ZeRO-3 - MAX GPU UTILIZATION\n\"\"\"\n\nimport os\nimport time\nimport json\nimport torch\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, TrainerCallback\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\n# Disable W&B\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# === DeepSpeed ZeRO-3 Config - MAX GPU UTILIZATION ===\nDEEPSPEED_CONFIG = {\n    \"bf16\": {\n        \"enabled\": True\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \n        # Keep optimizer on GPU if possible, overflow to CPU\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",  # Only optimizer to CPU (saves ~56GB)\n            \"pin_memory\": True,\n            \"buffer_count\": 8,\n            \"fast_init\": True\n        },\n        \n        # Keep parameters on GPU (fast)\n        \"offload_param\": {\n            \"device\": \"none\"  # NO param offload - max speed\n        },\n        \n        # Maximize GPU communication efficiency\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 1e9,  # Larger buckets = faster on H100\n        \"stage3_prefetch_bucket_size\": 1e9,\n        \"stage3_param_persistence_threshold\": 1e6,\n        \n        # Maximize memory usage\n        \"sub_group_size\": 1e12,\n        \"stage3_max_live_parameters\": 3e9,\n        \"stage3_max_reuse_distance\": 3e9,\n        \"stage3_gather_16bit_weights_on_model_save\": True\n    },\n    \n    # Gradient settings\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \n    # Batch settings\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \n    # Faster kernels\n    \"prescale_gradients\": False,\n    \"wall_clock_breakdown\": False,\n    \n    # Communication optimization for single GPU\n    \"communication_data_type\": \"bf16\",\n}\n\n\n# === Real-time Logger with GPU Stats ===\nclass RealTimeLogger(TrainerCallback):\n    def __init__(self):\n        self.start_time = None\n        self.step_times = []\n        \n    def on_train_begin(self, args, state, control, **kwargs):\n        self.start_time = time.time()\n        self.last_step_time = time.time()\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ TRAINING STARTED | DeepSpeed ZeRO-3 | MAX GPU UTILIZATION\")\n        print(\"=\"*80 + \"\\n\")\n        \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and \"loss\" in logs:\n            now = time.time()\n            step = state.global_step\n            total = state.max_steps\n            pct = (step / total) * 100 if total > 0 else 0\n            loss = logs.get(\"loss\", 0)\n            lr = logs.get(\"learning_rate\", 0)\n            epoch = logs.get(\"epoch\", 0)\n            \n            # Time calculations\n            elapsed = now - self.start_time\n            step_time = now - self.last_step_time\n            self.last_step_time = now\n            steps_per_sec = step / elapsed if elapsed > 0 else 0\n            remaining = (total - step) / steps_per_sec if steps_per_sec > 0 else 0\n            \n            # GPU stats\n            if torch.cuda.is_available():\n                mem_alloc = torch.cuda.memory_allocated() / 1e9\n                mem_reserved = torch.cuda.memory_reserved() / 1e9\n                mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n                gpu_util = (mem_reserved / mem_total) * 100\n                \n                # Get GPU compute utilization if available\n                try:\n                    import subprocess\n                    result = subprocess.run(\n                        ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],\n                        capture_output=True, text=True, timeout=1\n                    )\n                    compute_util = int(result.stdout.strip())\n                except:\n                    compute_util = -1\n            else:\n                gpu_util = 0\n                compute_util = -1\n            \n            # Format output\n            gpu_str = f\"Mem: {mem_reserved:.0f}/{mem_total:.0f}GB ({gpu_util:.0f}%)\"\n            if compute_util >= 0:\n                gpu_str += f\" | Compute: {compute_util}%\"\n            \n            print(f\"[{pct:5.1f}%] Step {step:>5}/{total} | \"\n                  f\"Loss: {loss:.4f} | LR: {lr:.2e} | \"\n                  f\"Epoch: {epoch:.2f} | {gpu_str} | \"\n                  f\"ETA: {timedelta(seconds=int(remaining))}\")\n    \n    def on_save(self, args, state, control, **kwargs):\n        print(f\"\\nüíæ Checkpoint saved at step {state.global_step}\\n\")\n    \n    def on_train_end(self, args, state, control, **kwargs):\n        elapsed = time.time() - self.start_time\n        print(\"\\n\" + \"=\"*80)\n        print(f\"‚úÖ TRAINING COMPLETE | Total time: {timedelta(seconds=int(elapsed))}\")\n        print(\"=\"*80 + \"\\n\")\n\n\n# === Configuration ===\nCONFIG = {\n    # Model\n    \"model_name\": \"microsoft/Phi-4-reasoning-plus\",\n    \"max_seq_length\": 4096,\n    \n    # Data\n    \"dataset_path\": \"/kaggle/input/aimath-train/data/sft_dataset.jsonl\",\n    \"text_field\": \"text\",\n    \n    # Training\n    \"output_dir\": \"/kaggle/working/outputs/sft\",\n    \"num_train_epochs\": 2,\n    \"per_device_train_batch_size\": 2,  # Increased for better GPU util\n    \"gradient_accumulation_steps\": 16,  # Adjusted to keep effective batch = 32\n    \n    # Optimizer\n    \"learning_rate\": 2e-5,\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.05,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \n    # Precision\n    \"bf16\": True,\n    \"tf32\": True,\n    \n    # Memory\n    \"gradient_checkpointing\": True,\n    \n    # Saving & Logging\n    \"save_steps\": 200,\n    \"save_total_limit\": 3,\n    \"logging_steps\": 5,\n    \n    # Misc\n    \"seed\": 42,\n}\n\n\ndef save_deepspeed_config():\n    \"\"\"Save DeepSpeed config to file.\"\"\"\n    config_path = Path(\"/kaggle/working/ds_config.json\")\n    with open(config_path, \"w\") as f:\n        json.dump(DEEPSPEED_CONFIG, f, indent=2)\n    print(f\"   ‚úÖ DeepSpeed config saved to {config_path}\")\n    return str(config_path)\n\n\ndef load_model_and_tokenizer():\n    \"\"\"Load Phi-4 for DeepSpeed ZeRO-3 training.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üì• Loading Model (DeepSpeed ZeRO-3 - Max GPU)\")\n    print(\"=\"*80)\n    \n    # Load tokenizer\n    print(\"\\n   Loading tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        CONFIG[\"model_name\"],\n        trust_remote_code=True,\n    )\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    print(f\"   ‚úÖ Tokenizer loaded | Vocab size: {len(tokenizer)}\")\n    \n    # Load model\n    print(\"\\n   Loading model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        CONFIG[\"model_name\"],\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        attn_implementation=\"flash_attention_2\",\n    )\n    \n    # Enable gradient checkpointing\n    if CONFIG[\"gradient_checkpointing\"]:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n        print(\"   ‚úÖ Gradient checkpointing enabled\")\n    \n    # Model stats\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"\\n   üìä Model Stats:\")\n    print(f\"      Total parameters:     {total_params / 1e9:.2f}B\")\n    print(f\"      Trainable parameters: {trainable_params / 1e9:.2f}B\")\n    \n    return model, tokenizer\n\n\ndef load_sft_dataset(path: str):\n    \"\"\"Load dataset from JSONL file.\"\"\"\n    print(f\"\\nüìÇ Loading dataset from {path}\")\n    dataset = load_dataset(\"json\", data_files=path, split=\"train\")\n    print(f\"   ‚úÖ Loaded {len(dataset):,} examples\")\n    return dataset\n\n\ndef create_training_arguments(deepspeed_config_path: str):\n    \"\"\"Create training arguments with DeepSpeed.\"\"\"\n    \n    output_dir = Path(CONFIG[\"output_dir\"])\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    return TrainingArguments(\n        # Output\n        output_dir=str(output_dir),\n        \n        # Training duration\n        num_train_epochs=CONFIG[\"num_train_epochs\"],\n        \n        # Batch size - LARGER for better GPU utilization\n        per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n        \n        # Learning rate\n        learning_rate=CONFIG[\"learning_rate\"],\n        lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n        warmup_ratio=CONFIG[\"warmup_ratio\"],\n        weight_decay=CONFIG[\"weight_decay\"],\n        max_grad_norm=CONFIG[\"max_grad_norm\"],\n        \n        # Precision\n        bf16=CONFIG[\"bf16\"],\n        tf32=CONFIG[\"tf32\"],\n        \n        # Gradient checkpointing\n        gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n        \n        # DeepSpeed\n        deepspeed=deepspeed_config_path,\n        \n        # Saving\n        save_strategy=\"steps\",\n        save_steps=CONFIG[\"save_steps\"],\n        save_total_limit=CONFIG[\"save_total_limit\"],\n        \n        # Logging\n        logging_steps=CONFIG[\"logging_steps\"],\n        logging_first_step=True,\n        report_to=\"none\",\n        disable_tqdm=False,\n        \n        # Speed optimizations\n        dataloader_num_workers=4,\n        dataloader_pin_memory=True,\n        dataloader_prefetch_factor=2,\n        \n        # Misc\n        seed=CONFIG[\"seed\"],\n        local_rank=-1,\n    )\n\n\ndef print_training_summary(dataset, training_args):\n    \"\"\"Print training configuration summary.\"\"\"\n    \n    effective_batch = (\n        training_args.per_device_train_batch_size *\n        training_args.gradient_accumulation_steps\n    )\n    steps_per_epoch = len(dataset) // effective_batch\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    \n    # Faster with batch_size=2 (~5s per step)\n    estimated_time = total_steps * 5\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üìã Training Configuration - MAX GPU UTILIZATION\")\n    print(\"=\"*80)\n    print(f\"   Model:              {CONFIG['model_name']}\")\n    print(f\"   Mode:               Full Fine-Tuning + DeepSpeed ZeRO-3\")\n    print(f\"   Dataset size:       {len(dataset):,}\")\n    print(f\"   Max seq length:     {CONFIG['max_seq_length']}\")\n    print(f\"   Epochs:             {training_args.num_train_epochs}\")\n    print(f\"   Micro batch size:   {training_args.per_device_train_batch_size}\")\n    print(f\"   Gradient accum:     {training_args.gradient_accumulation_steps}\")\n    print(f\"   Effective batch:    {effective_batch}\")\n    print(f\"   Steps per epoch:    {steps_per_epoch:,}\")\n    print(f\"   Total steps:        {total_steps:,}\")\n    print(f\"   Learning rate:      {training_args.learning_rate}\")\n    print(f\"   Precision:          bf16 + tf32\")\n    print(f\"   Param offload:      None (GPU)\")\n    print(f\"   Optimizer offload:  CPU\")\n    print(f\"   Grad checkpointing: {CONFIG['gradient_checkpointing']}\")\n    print(f\"\\n   ‚è±Ô∏è  Estimated time:  {timedelta(seconds=estimated_time)}\")\n    print(\"=\"*80)\n\n\ndef main():\n    \"\"\"Main training function.\"\"\"\n    start_time = time.time()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üéØ Phi-4 Math SFT Training - MAX GPU UTILIZATION\")\n    print(\"=\"*80)\n    print(f\"   Started at:  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"   Platform:    Kaggle H100 80GB\")\n    print(f\"   Mode:        Full Fine-Tuning (14B params)\")\n    print(f\"   Strategy:    DeepSpeed ZeRO-3 | Params on GPU | Optimizer on CPU\")\n    \n    # GPU info\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0)\n        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"   GPU:         {gpu_name} ({gpu_mem:.0f} GB)\")\n    \n    # Save DeepSpeed config\n    print(\"\\n‚öôÔ∏è  Setting up DeepSpeed ZeRO-3...\")\n    ds_config_path = save_deepspeed_config()\n    \n    # Load model\n    model, tokenizer = load_model_and_tokenizer()\n    \n    # Load dataset\n    dataset = load_sft_dataset(CONFIG[\"dataset_path\"])\n    \n    # Create training arguments\n    training_args = create_training_arguments(ds_config_path)\n    \n    # Print summary\n    print_training_summary(dataset, training_args)\n    \n    # Create trainer\n    print(\"\\nüèãÔ∏è Creating SFT Trainer...\")\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset,\n        args=training_args,\n        dataset_text_field=CONFIG[\"text_field\"],\n        max_seq_length=CONFIG[\"max_seq_length\"],\n        packing=True,\n        callbacks=[RealTimeLogger()],\n    )\n    \n    # Pre-training GPU check\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        mem_alloc = torch.cuda.memory_allocated() / 1e9\n        mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"\\nüìä GPU Memory before training: {mem_alloc:.1f} / {mem_total:.0f} GB\")\n    \n    # Train\n    trainer.train()\n    \n    # Save final model\n    print(\"\\nüíæ Saving final model...\")\n    final_path = Path(CONFIG[\"output_dir\"]) / \"final\"\n    trainer.save_model(str(final_path))\n    tokenizer.save_pretrained(str(final_path))\n    \n    # Summary\n    elapsed = time.time() - start_time\n    print(f\"\\n‚úÖ Model saved to: {final_path}\")\n    print(f\"   Total training time: {timedelta(seconds=int(elapsed))}\")\n    \n    return str(final_path)\n\n\nif __name__ == \"__main__\":\n    final_model_path = main()\n    print(f\"\\nüéâ SFT Complete! Model at: {final_model_path}\")\n    print(f\"   Next step: Run GRPO training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:34:13.293063Z","iopub.execute_input":"2025-11-30T12:34:13.29318Z","iopub.status.idle":"2025-11-30T12:34:26.863481Z","shell.execute_reply.started":"2025-11-30T12:34:13.293167Z","shell.execute_reply":"2025-11-30T12:34:26.862809Z"}},"outputs":[{"name":"stderr","text":"2025-11-30 12:34:21.493193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764506061.509285    2529 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764506061.514742    2529 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nüéØ Phi-4 Math SFT Training - MAX GPU UTILIZATION\n================================================================================\n   Started at:  2025-11-30 12:34:25\n   Platform:    Kaggle H100 80GB\n   Mode:        Full Fine-Tuning (14B params)\n   Strategy:    DeepSpeed ZeRO-3 | Params on GPU | Optimizer on CPU\n   GPU:         NVIDIA H100 80GB HBM3 (85 GB)\n\n‚öôÔ∏è  Setting up DeepSpeed ZeRO-3...\n   ‚úÖ DeepSpeed config saved to /kaggle/working/ds_config.json\n\n================================================================================\nüì• Loading Model (DeepSpeed ZeRO-3 - Max GPU)\n================================================================================\n\n   Loading tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Tokenizer loaded | Vocab size: 100352\n\n   Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7c874e98e314dbcb20bac32db10ad41"}},"metadata":{}},{"name":"stdout","text":"   ‚úÖ Gradient checkpointing enabled\n\n   üìä Model Stats:\n      Total parameters:     14.66B\n      Trainable parameters: 14.66B\n\nüìÇ Loading dataset from /kaggle/working/data/sft_dataset.jsonl\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2529/1793213026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0mfinal_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüéâ SFT Complete! Model at: {final_model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Next step: Run GRPO training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_2529/1793213026.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sft_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;31m# Create training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_2529/1793213026.py\u001b[0m in \u001b[0;36mload_sft_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;34m\"\"\"Load dataset from JSONL file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìÇ Loading dataset from {path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   ‚úÖ Loaded {len(dataset):,} examples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1398\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         )\n\u001b[0;32m--> 527\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    706\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 598\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    599\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/kaggle/working/data/sft_dataset.jsonl'"],"ename":"FileNotFoundError","evalue":"Unable to find '/kaggle/working/data/sft_dataset.jsonl'","output_type":"error"}],"execution_count":2}]}